{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing Models with PyTorch\n",
    "\n",
    "Quantizing an NLP-based model in PyTorch involves reducing the precision of the model's parameters to improve its inference speed and reduce its memory footprint. The process involves converting floating-point parameters to integers and can be implemented by adding a few lines of code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "\n",
    "The first step is to load any NLP-related model. In this notebook, we will be using a pre-trained GPT-2 model from the Hugging Face's Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Quantization (PTQ)\n",
    "\n",
    "Post-Training Quantization (PTQ) is a technique of quantizing a pre-trained model, where dynamic quantization is used to adjust the quantization levels during runtime to ensure optimal accuracy and performance.\n",
    "\n",
    "Archai's offer a wrapper function, denoted as `dynamic_quantization_torch()`, which takes care of dynamically quantizing the pre-trained model.\n",
    "\n",
    "*Note that we set PyTorch's number of threads to 1 because quantized models will only use a single thread.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-03 12:05:52,756 - archai.quantization.ptq — INFO —  Quantizing model ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from archai.quantization.ptq import dynamic_quantization_torch\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "model_qnt = dynamic_quantization_torch(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Default and Quantized Models\n",
    "\n",
    "Finally, we can compare the size of default and quantized models, as well as their logits different. Nevertheless, please note that if the model has not been pre-trained with Quantization Aware Training (QAT), it might produce different logits and have its performance diminished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 510.391647MB\n",
      "Model-QNT: 431.250044MB\n",
      "Difference between logits: tensor([[[ -0.3091,  -0.5829,  -0.1439,  ...,   3.1061,   2.7097,  -1.1030],\n",
      "         [ -1.3238,  -0.7332,  -3.8590,  ...,  -2.8122,  -3.3422,  -1.6324],\n",
      "         [ -2.3850,  -5.1132,  -6.7728,  ...,  -4.2977,  -4.5302,  -1.9685],\n",
      "         ...,\n",
      "         [ -1.6885,  -5.1900,  -9.1044,  ...,   1.7422,  -1.2876,   0.9441],\n",
      "         [ -5.2036,  -8.5287, -11.4208,  ...,  -3.6595,  -5.0663,  -2.8279],\n",
      "         [ -4.3205,  -7.2593, -10.5583,  ...,  -2.6262,  -3.7815,  -1.0048]]],\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from archai.common.file_utils import calculate_torch_model_size\n",
    "\n",
    "print(f\"Model: {calculate_torch_model_size(model)}MB\")\n",
    "print(f\"Model-QNT: {calculate_torch_model_size(model_qnt)}MB\")\n",
    "\n",
    "inputs = {\"input_ids\": torch.randint(1, 10, (1, 192))}\n",
    "logits = model(**inputs).logits\n",
    "logits_qnt = model_qnt(**inputs).logits\n",
    "\n",
    "print(f\"Difference between logits: {logits_qnt - logits}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
