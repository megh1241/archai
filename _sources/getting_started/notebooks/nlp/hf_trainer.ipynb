{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NLP-based Models with Hugging Face\n",
    "\n",
    "Training an NLP-based model involves several steps, including loading the data, encoding the data, defining the model architecture, and conducting the actual training process.\n",
    "\n",
    "Archai implements abstract base classes that defines the expected behavior of some classes, such as datasets (`DatasetProvider`) and trainers (`TrainerBase`). Additionally, we offer boilerplate classes for the most common frameworks, such as a `DatasetProvider` compatible with `huggingface/datasets` and a `TrainerBase` compatible with `huggingface/transformers`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Encoding the Data\n",
    "\n",
    "When using a dataset provider, such as Hugging Face's `datasets` library, the data loading process is simplified, as the provider takes care of downloading and pre-processing the required dataset. Next, the data needs to be encoded, typically by converting text data into numerical representations that can be fed into the model. \n",
    "\n",
    "This step is accomplished in the same way as the [previous notebook](./hf_dataset_provider.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/gderosa/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at C:\\Users\\gderosa\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-04d7ff93d438ade6.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from archai.datasets.nlp.hf_dataset_provider import HfHubDatasetProvider\n",
    "from archai.datasets.nlp.hf_dataset_provider_utils import tokenize_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\", model_max_length=1024)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "dataset_provider = HfHubDatasetProvider(\"wikitext\", dataset_config_name=\"wikitext-103-raw-v1\")\n",
    "\n",
    "# When loading `train_dataset`, we will override the split argument to only load 1%\n",
    "# of the data and speed up its encoding\n",
    "train_dataset = dataset_provider.get_train_dataset(split=\"train[:1%]\")\n",
    "encoded_train_dataset = train_dataset.map(tokenize_dataset, batched=True, fn_kwargs={\"tokenizer\": tokenizer})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "Once the data is encoded, we can define any NLP-based model. In this example, we will use a CodeGen architecture from `huggingface/transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CodeGenConfig, CodeGenForCausalLM\n",
    "\n",
    "config = CodeGenConfig(\n",
    "    n_positions=1024,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    rotary_dim=16,\n",
    "    bos_token_id=0,\n",
    "    eos_token_id=0,\n",
    "    vocab_size=50295,\n",
    ")\n",
    "model = CodeGenForCausalLM(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Trainer\n",
    "\n",
    "The final step is to use the Hugging Face trainer abstraction (`HfTrainer`) to conduct the training process, which involves optimizing the model's parameters using a pre-defined optimization algorithm and loss function, and updating the model's parameters based on the training data. This process is repeated until the model converges to a satisfactory accuracy or performance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "The following columns in the training set don't have a corresponding argument in `CodeGenForCausalLM.forward` and have been ignored: text. If text are not expected by `CodeGenForCausalLM.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\gderosa\\Anaconda3\\envs\\archai\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 18014\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 162304119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8600bd55454d42bd9f1e824f41e8e532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\gderosa\\Anaconda3\\envs\\archai\\lib\\site-packages\\transformers\\models\\codegen\\modeling_codegen.py:166: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorCompare.cpp:413.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric loss:\n",
      "11.0034\n",
      "Attempted to log scalar metric learning_rate:\n",
      "0.0\n",
      "Attempted to log scalar metric epoch:\n",
      "0.0\n",
      "{'loss': 11.0034, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "Attempted to log scalar metric train_runtime:\n",
      "10.6794\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "0.094\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "0.094\n",
      "Attempted to log scalar metric total_flos:\n",
      "759874922496.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "11.003437995910645\n",
      "Attempted to log scalar metric epoch:\n",
      "0.0\n",
      "{'train_runtime': 10.6794, 'train_samples_per_second': 0.094, 'train_steps_per_second': 0.094, 'train_loss': 11.003437995910645, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=11.003437995910645, metrics={'train_runtime': 10.6794, 'train_samples_per_second': 0.094, 'train_steps_per_second': 0.094, 'train_loss': 11.003437995910645, 'epoch': 0.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from archai.trainers.nlp.hf_trainer import HfTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"hf-codegen\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.1,\n",
    "    max_steps=1,\n",
    ")\n",
    "trainer = HfTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=encoded_train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
