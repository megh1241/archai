{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Custom Trainer\n",
    "\n",
    "Abstract base classes (ABCs) define a blueprint for a class, specifying its methods and attributes, but not its implementation. They are important in implementing a consistent interface, as they enforce a set of requirements on implementing classes and make it easier to write code that can work with multiple implementations.\n",
    "\n",
    "First, we define a boilerplate for the `TrainerBase` class, which is the same implemented in `archai.api.trainer_base` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "from overrides import EnforceOverrides\n",
    "\n",
    "\n",
    "class TrainerBase(EnforceOverrides):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch-based Trainer\n",
    "\n",
    "In the context of a custom trainer, using ABCs can help ensure that the provider implements the required methods and provides a consistent interface for training, evaluating and predicting. In this example, we will implement a PyTorch-based trainer, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from overrides import overrides\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PyTorchTrainer(TrainerBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        # Setup the trainer\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self) -> None:\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "\n",
    "    def _train_step(self, inputs: torch.Tensor, labels: torch.Tensor) -> None:\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        outputs = self.model(inputs)\n",
    "\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    @overrides\n",
    "    def train(self) -> None:\n",
    "        total_loss = 0.0\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(self.train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            total_loss += self._train_step(inputs, labels)\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Batch {idx} loss: {total_loss / (idx + 1)}\")\n",
    "\n",
    "    def _eval_step(self, inputs: torch.Tensor, labels: torch.Tensor) -> None:\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    @overrides\n",
    "    def evaluate(self, eval_dataset: Optional[Dataset] = None) -> None:\n",
    "        eval_dataset = eval_dataset if eval_dataset else self.eval_dataset\n",
    "        assert eval_dataset is not None, \"`eval_dataset` has not been provided.\"\n",
    "\n",
    "        eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        self.model.eval()\n",
    "        for idx, (inputs, labels) in enumerate(eval_loader):\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            loss = self._eval_step(inputs, labels)\n",
    "\n",
    "            eval_loss += loss\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        eval_loss /= idx\n",
    "\n",
    "        return eval_loss\n",
    "\n",
    "    @overrides\n",
    "    def predict(self, inputs: torch.Tensor) -> None:\n",
    "        self.model.eval()\n",
    "        preds = self.model(inputs)\n",
    "        self.model.train()\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "Once the data is loaded, we can define any CV-based model. In this example, we will create a simple linear model using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Training with the Trainer\n",
    "\n",
    "After loading the data and creating the data, we need to plug these instances into the `PyTorchTrainer` and start the training, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 loss: 2.3260679244995117\n",
      "Batch 10 loss: 2.170200304551558\n",
      "Batch 20 loss: 2.013322977792649\n",
      "Batch 30 loss: 1.88041107116207\n",
      "Batch 40 loss: 1.7696490898364927\n",
      "Batch 50 loss: 1.6737279284234141\n",
      "Batch 60 loss: 1.580565809226427\n",
      "Batch 70 loss: 1.5032367882594255\n",
      "Batch 80 loss: 1.4377957427943195\n",
      "Batch 90 loss: 1.3756403844435137\n",
      "Batch 100 loss: 1.323841373519142\n",
      "Batch 110 loss: 1.2777430780299075\n",
      "Batch 120 loss: 1.2366919005212704\n",
      "Batch 130 loss: 1.1966573777999587\n",
      "Batch 140 loss: 1.1582764298357862\n",
      "Batch 150 loss: 1.1270605416487385\n",
      "Batch 160 loss: 1.0991555715199584\n",
      "Batch 170 loss: 1.071832075105076\n",
      "Batch 180 loss: 1.0470546456002399\n",
      "Batch 190 loss: 1.0244618591837857\n",
      "Batch 200 loss: 1.0021289893940313\n",
      "Batch 210 loss: 0.9831973584059855\n",
      "Batch 220 loss: 0.9649985725253956\n",
      "Batch 230 loss: 0.9455169081945956\n",
      "Batch 240 loss: 0.9282616229720135\n",
      "Batch 250 loss: 0.911252826689724\n",
      "Batch 260 loss: 0.894186370674221\n",
      "Batch 270 loss: 0.8811650495027704\n",
      "Batch 280 loss: 0.8695434934303854\n",
      "Batch 290 loss: 0.8567541093146268\n",
      "Batch 300 loss: 0.8456319903218469\n",
      "Batch 310 loss: 0.8329309043010331\n",
      "Batch 320 loss: 0.8228098772396552\n",
      "Batch 330 loss: 0.8110202938048141\n",
      "Batch 340 loss: 0.8010669230016445\n",
      "Batch 350 loss: 0.7917746302069422\n",
      "Batch 360 loss: 0.7834661878707336\n",
      "Batch 370 loss: 0.7756232653024062\n",
      "Batch 380 loss: 0.7670649672587087\n",
      "Batch 390 loss: 0.7599971420167352\n",
      "Batch 400 loss: 0.7524218939040367\n",
      "Batch 410 loss: 0.7447285842866503\n",
      "Batch 420 loss: 0.7372030290175504\n",
      "Batch 430 loss: 0.7303863203027685\n",
      "Batch 440 loss: 0.7235838531636868\n",
      "Batch 450 loss: 0.7160437242551283\n",
      "Batch 460 loss: 0.7095201176443742\n",
      "Batch 470 loss: 0.7035116710227513\n",
      "Batch 480 loss: 0.6981078017030585\n",
      "Batch 490 loss: 0.6921659559921918\n",
      "Batch 500 loss: 0.6869516342342971\n",
      "Batch 510 loss: 0.6813836557874474\n",
      "Batch 520 loss: 0.6757925200256414\n",
      "Batch 530 loss: 0.6707233819977263\n",
      "Batch 540 loss: 0.6653567692764145\n",
      "Batch 550 loss: 0.660618505466006\n",
      "Batch 560 loss: 0.6552219002004613\n",
      "Batch 570 loss: 0.6498897406036089\n",
      "Batch 580 loss: 0.6457371746098524\n",
      "Batch 590 loss: 0.6425109619870404\n",
      "Batch 600 loss: 0.6391324787439403\n",
      "Batch 610 loss: 0.6355814298465482\n",
      "Batch 620 loss: 0.6322107954061742\n",
      "Batch 630 loss: 0.6274927602328135\n",
      "Batch 640 loss: 0.6241415665134811\n",
      "Batch 650 loss: 0.6207633239607657\n",
      "Batch 660 loss: 0.6171814470626583\n",
      "Batch 670 loss: 0.6138749045782047\n",
      "Batch 680 loss: 0.61017940756858\n",
      "Batch 690 loss: 0.6062270997876541\n",
      "Batch 700 loss: 0.6035018302021965\n",
      "Batch 710 loss: 0.6011713738389491\n",
      "Batch 720 loss: 0.5979135201600653\n",
      "Batch 730 loss: 0.5938765308354687\n",
      "Batch 740 loss: 0.590924397412582\n",
      "Batch 750 loss: 0.5882599428792133\n",
      "Batch 760 loss: 0.5857056667942568\n",
      "Batch 770 loss: 0.5834706003177336\n",
      "Batch 780 loss: 0.5803632058689749\n",
      "Batch 790 loss: 0.5780422753653243\n",
      "Batch 800 loss: 0.5748808927899145\n",
      "Batch 810 loss: 0.5718222048441373\n",
      "Batch 820 loss: 0.5691325509882148\n",
      "Batch 830 loss: 0.5673209575826367\n",
      "Batch 840 loss: 0.5649140686611783\n",
      "Batch 850 loss: 0.562490068244738\n",
      "Batch 860 loss: 0.5596906001248011\n",
      "Batch 870 loss: 0.5576014952257224\n",
      "Batch 880 loss: 0.5552688866650747\n",
      "Batch 890 loss: 0.553324693130323\n",
      "Batch 900 loss: 0.5513346120963218\n",
      "Batch 910 loss: 0.5494838998981155\n",
      "Batch 920 loss: 0.5473718988940718\n",
      "Batch 930 loss: 0.5448646569418472\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from archai.datasets.cv.mnist_dataset_provider import MnistDatasetProvider\n",
    "\n",
    "dataset_provider = MnistDatasetProvider()\n",
    "train_dataset = dataset_provider.get_train_dataset()\n",
    "\n",
    "trainer = PyTorchTrainer(model, train_dataset=train_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating and Predicting with the Trainer\n",
    "\n",
    "Finally, we evaluate our pre-trained model with the validation set and create a set of random-based inputs to calculate the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss: 0.3327318702896054\n",
      "Predictions: tensor([[-0.1380,  0.1920, -0.0317, -0.0902,  0.0679,  0.1481, -0.0149,  0.0998,\n",
      "         -0.2689, -0.0635]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = dataset_provider.get_val_dataset()\n",
    "\n",
    "eval_loss  = trainer.evaluate(eval_dataset=val_dataset)\n",
    "print(f\"Eval loss: {eval_loss}\")\n",
    "\n",
    "inputs = torch.zeros(1, 28 * 28)\n",
    "preds = trainer.predict(inputs)\n",
    "print(f\"Predictions: {preds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
