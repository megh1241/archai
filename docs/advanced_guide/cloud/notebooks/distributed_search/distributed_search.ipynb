{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Search\n",
    "\n",
    "This notebook and accompanying code shows how to run an Archai Network Architecture Search (NAS) using\n",
    "an Azure Machine Learning Workspace with distributed partial training of models on a GPU cluster.\n",
    "This example requires a storage account and Azure machine learning workspace specified in a config.json\n",
    "file like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"subscription_id\": \"...\",\n",
    "    \"resource_group\": \"...\",\n",
    "    \"workspace_name\": \"...\",\n",
    "    \"storage_account_key\": \"...\",\n",
    "    \"storage_account_name\": \"...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using storage account: archaimnistmodels\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import archai.common.azureml_helper as aml_helper\n",
    "from azure.ai.ml.entities import UserIdentityConfiguration\n",
    "\n",
    "sys.path.append(os.path.realpath('scripts'))\n",
    "\n",
    "# make sure we have a scripts dir for the code to run our jobs.\n",
    "import os\n",
    "scripts_dir = \"./scripts\"\n",
    "os.makedirs(scripts_dir, exist_ok=True)\n",
    "\n",
    "config_file = \"../.azureml/config.json\"\n",
    "config = json.load(open(config_file, 'r'))\n",
    "\n",
    "for required_key in ['subscription_id', 'resource_group', 'workspace_name', 'storage_account_key', 'storage_account_name']:\n",
    "    if not required_key in config:\n",
    "        print(f\"### Error: please add a {required_key} to {config_file}\")\n",
    "\n",
    "storage_account_key = config['storage_account_key']    \n",
    "storage_account_name = config['storage_account_name']\n",
    "\n",
    "print(f'Using storage account: {storage_account_name}')\n",
    "\n",
    "# This is the AML experiment name \n",
    "experiment_name = 'mnist_test_run'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our [search.py](scripts/search.py) will use a ConfigSearchSpace based on the following parameters, the model defined in [model.py](scripts/model.py) will take various configurations and build different shape CNN models for each configuration as shown below. Each time you execute this cell it will generate a new random CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArchConfig({\n",
      "    \"nb_layers\": 1,\n",
      "    \"kernel_size\": 1,\n",
      "    \"hidden_dim\": 16\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (4): Conv2d(16, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from archai.discrete_search.search_spaces.config import ArchParamTree, DiscreteChoice\n",
    "from model import MyModel\n",
    "\n",
    "arch_param_tree = ArchParamTree({\n",
    "    'nb_layers': DiscreteChoice(list(range(1, 13))),\n",
    "    'kernel_size': DiscreteChoice([1, 3, 5, 7]),\n",
    "    'hidden_dim': DiscreteChoice([16, 32, 64, 128])\n",
    "})\n",
    "\n",
    "arch_config = arch_param_tree.sample_config()\n",
    "print(arch_config)\n",
    "\n",
    "MyModel(arch_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a handle to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ..\\.azureml\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using workspace \"snpe-aml-workspace\" in resource group \"snpe-aml-rg\"\n"
     ]
    }
   ],
   "source": [
    "ml_client = aml_helper.get_aml_client_from_file(config_path=config_file)\n",
    "print(f'Using workspace \"{ml_client.workspace_name}\" in resource group \"{ml_client.resource_group_name}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the compute clusters that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already have a cluster named nas-cpu-cluster-D14-v2, we'll reuse it as is.\n",
      "You already have a cluster named nas-gpu-cluster-NC6, we'll reuse it as is.\n"
     ]
    }
   ],
   "source": [
    "# Create cpu cluster for running the search\n",
    "cpu_compute_name = \"nas-cpu-cluster-D14-v2\"\n",
    "aml_helper.create_compute_cluster(ml_client, cpu_compute_name, size=\"Standard_D14_v2\", location=\"westus2\")\n",
    "\n",
    "# Create gpu cluster for running the search\n",
    "gpu_compute_name = \"nas-gpu-cluster-NC6\"\n",
    "aml_helper.create_compute_cluster(ml_client, gpu_compute_name, size=\"Standard_NC6\", location=\"westus2\", max_instances=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the AML Environment from our conda.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name aml-archai is registered to workspace, the environment version is 0.1.7\n"
     ]
    }
   ],
   "source": [
    "archai_job_env = aml_helper.create_environment_from_file(ml_client, conda_file=\"conda.yaml\", version='0.1.7')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure our storage account is setup with `models` blob store container and `status` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from store import ArchaiStore\n",
    "\n",
    "# Register the datastore with AML\n",
    "data_store_name = \"datasets\"\n",
    "data_container_name = \"datasets\"\n",
    "model_store_name = \"models\"\n",
    "model_container_name = \"models\"\n",
    "root_folder = experiment_name\n",
    "\n",
    "# make sure the datasets container exists\n",
    "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=data_container_name)\n",
    "store.upload_blob(root_folder, config_file)\n",
    "\n",
    "# make sure the models container exists\n",
    "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=model_container_name)\n",
    "store.upload_blob(\"config\", config_file)\n",
    "\n",
    "datastore_path = f'azureml://datastores/{data_store_name}/paths/{root_folder}'\n",
    "results_path = f'azureml://datastores/{model_store_name}/paths/{root_folder}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the `models` blob store container in the ML workspace `datastores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AzureBlobDatastore\n",
    "from azure.ai.ml.entities._credentials import AccountKeyConfiguration\n",
    "\n",
    "try:\n",
    "    model_store = ml_client.datastores.get(model_store_name)\n",
    "except:    \n",
    "    model_store = AzureBlobDatastore(\n",
    "        name=model_store_name,\n",
    "        description=\"Datastore pointing to our models blob container.\",\n",
    "        account_name=storage_account_name,\n",
    "        container_name=model_container_name,\n",
    "        credentials=AccountKeyConfiguration(\n",
    "            account_key=storage_account_key\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ml_client.create_or_update(model_store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the `datasets` blob store container in the ML workspace `datastores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_store = ml_client.datastores.get(data_store_name)\n",
    "except:\n",
    "    data_store = AzureBlobDatastore(\n",
    "        name=data_store_name,\n",
    "        description=\"Datastore pointing to our dataset container.\",\n",
    "        account_name=storage_account_name,\n",
    "        container_name=data_container_name,\n",
    "        credentials=AccountKeyConfiguration(\n",
    "            account_key=storage_account_key\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ml_client.create_or_update(data_store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline command that prepares our MNIST dataset using `prep_data_store.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_prep_component = command(\n",
    "    name=\"data_prep2\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Downloads the remote dataset to our blob store.\",\n",
    "    inputs= {\n",
    "        \"name\": Input(type='string')\n",
    "    },\n",
    "    outputs= {\n",
    "        \"data\": Output(type=\"uri_folder\", path=datastore_path, mode=\"rw_mount\")\n",
    "    },\n",
    "\n",
    "    # The source folder of the component\n",
    "    code=scripts_dir,\n",
    "    command=\"\"\"python3 prep_data_store.py \\\n",
    "            --path ${{outputs.data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{archai_job_env.name}:{archai_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a command that kids of the Archai Search using `search.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--data_dir azureml://datastores/datasets/paths/mnist_test_run --output_dir azureml://datastores/models/paths/mnist_test_run --environment \"aml-archai:0.1.7\" --experiment \"mnist_test_run\" --compute \"nas-gpu-cluster-NC6\" --config \"7b22737562736372697074696f6e5f6964223a202263386237663931332d363066622d343735392d613331302d666335363330653536663939222c20227265736f757263655f67726f7570223a2022736e70652d616d6c2d7267222c2022776f726b73706163655f6e616d65223a2022736e70652d616d6c2d776f726b7370616365222c2022696f745f7265736f757263655f67726f7570223a2022736e70652d6465766963652d6875622d7267222c2022696f745f6875625f6e616d65223a20224d7372536e7065446576696365487562222c20226c6f636174696f6e223a202277657374757332222c202273746f726167655f6163636f756e745f6b6579223a202279485a4459454c3045774a65754c54517747395047713867564f6d78777031593836686e54367239735732666659535967686f4c496a694973712f4353454e45766471785a78546b713872482b4153747548445944773d3d222c202273746f726167655f6163636f756e745f6e616d65223a20226172636861696d6e6973746d6f64656c73227d\" --partial_training_epochs 0.1 --full_training_epochs 10 \n"
     ]
    }
   ],
   "source": [
    "environment_name = f'{archai_job_env.name}:{archai_job_env.version}'\n",
    "hex_config = bytes(json.dumps(config), encoding='utf-8').hex()\n",
    "\n",
    "partial_epochs = 0.1\n",
    "full_epochs = 10\n",
    "\n",
    "output_path = results_path + '/' + experiment_name\n",
    "fixed_args = f'--data_dir {datastore_path} ' + \\\n",
    "             f'--output_dir {results_path} ' + \\\n",
    "             f'--environment \"{environment_name}\" ' + \\\n",
    "             f'--experiment \"{experiment_name}\" ' + \\\n",
    "             f'--compute \"{gpu_compute_name}\" ' + \\\n",
    "             f'--config \"{hex_config}\" ' + \\\n",
    "             f'--partial_training_epochs {partial_epochs} ' + \\\n",
    "             f'--full_training_epochs {full_epochs} '\n",
    "\n",
    "print(fixed_args)\n",
    "\n",
    "search_component = command(\n",
    "    name=\"search\",\n",
    "    display_name=\"The Archai NAS search\",\n",
    "    description=\"Runs the NAS search algorithm.\",    \n",
    "    inputs= {\n",
    "        \"data\": Input(type=\"uri_folder\")\n",
    "    },\n",
    "    outputs= {\n",
    "        \"results\": Output(type=\"uri_folder\", path=output_path, mode=\"rw_mount\")\n",
    "    },\n",
    "    code=scripts_dir,\n",
    "    identity= UserIdentityConfiguration(),\n",
    "    command='python3 search.py --local_output ${{outputs.results}} ' + \\\n",
    "        fixed_args,\n",
    "    environment=f\"{archai_job_env.name}:{archai_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an AML pipeline with the data prep and search components piping the output of the data prep to the search component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_name,\n",
    "    description=\"Data prep pipeline\",\n",
    ")\n",
    "def mnist_search_pipeline():\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep_component(\n",
    "        name=\"MNIST\"\n",
    "    )\n",
    "\n",
    "    # check the dataset\n",
    "    check_job = search_component(\n",
    "        data=data_prep_job.outputs.data\n",
    "    )\n",
    "    \n",
    "    return { \"data\": data_prep_job.outputs.data }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = mnist_search_pipeline()\n",
    "\n",
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=experiment_name,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the pipeline azure ML studio portal in your web browser (This works when you are running this notbook in VS code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(pipeline_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Take a look at the pareto curve plots.  This cell can be run multiple times and you will see updates as each iteration finishes.\n",
    "You can even run this later after restarting the jupyter notebook because it is not dependent on variable state it is only\n",
    "dependent on the persistent 'models' blob store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching results for mnist_test_run...\n"
     ]
    }
   ],
   "source": [
    "from results import get_results, show_results, download_best_models\n",
    "from store import ArchaiStore\n",
    "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=model_container_name)\n",
    "\n",
    "print(f'Fetching results for {experiment_name}...')\n",
    "blob_path = root_folder + '/' + experiment_name\n",
    "output_folder = experiment_name\n",
    "\n",
    "get_results(store, blob_path, output_folder)\n",
    "show_results(output_folder)\n",
    "download_best_models(store, experiment_name, output_folder)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ONNX Runtime Inference on the Best Model\n",
    "\n",
    "When the search pipeline completes you should have a `top_models.json` file in the experiment_name output folder and you can use that to find the most accurate model and run it through the ONNX runtime to see if the ONNX inference gets the same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Could not find mnist_test_run\\top_models.json file. Please wait for job to finish.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(filename):\n\u001b[1;32m----> 7\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not find \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m file. Please wait for job to finish.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m results \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[1;31mException\u001b[0m: Could not find mnist_test_run\\top_models.json file. Please wait for job to finish."
     ]
    }
   ],
   "source": [
    "# find the top model in the json results.\n",
    "filename = os.path.join(output_folder, \"top_models.json\")\n",
    "best_of_the_best = None\n",
    "top_accuracy = 0\n",
    "row = None\n",
    "if not os.path.isfile(filename):\n",
    "    raise Exception(f\"Could not find {filename} file. Please wait for job to finish.\")\n",
    "\n",
    "results = json.load(open(filename, \"r\"))\n",
    "for key in results.keys():\n",
    "    a = results[key]\n",
    "    if type(a) is dict and 'val_acc' in a:\n",
    "        val_acc = a['val_acc']\n",
    "        if val_acc > top_accuracy:\n",
    "            top_accuracy = val_acc\n",
    "            best_of_the_best = key\n",
    "            row = a\n",
    "\n",
    "print(f\"The top model is {best_of_the_best} with accuracy {top_accuracy} and architecture {row['archid']}\")\n",
    "\n",
    "blob_path = root_folder + '/' + best_of_the_best\n",
    "model_output = os.path.join(output_folder, 'top_model')\n",
    "get_results(store, blob_path, model_output)\n",
    "\n",
    "model_path = os.path.join(model_output, 'model.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great, now let's test if this model works as advertised.\n",
    "from archai.datasets.cv.mnist_dataset_provider import MnistDatasetProvider\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "dataset_provider = MnistDatasetProvider()\n",
    "val_data = dataset_provider.get_val_dataset()\n",
    "count = val_data.data.shape[0]\n",
    "test = np.random.choice(count, 1)[0]\n",
    "data = val_data.data[test]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check what the images look like.\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(data, cmap='gray')\n",
    "print(f'data has shape: {data.shape}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Now run the ONNX runtime on this the validation set.\n",
    "# You can change this to `CUDAExecutionProvider` if you have a GPU and have\n",
    "# installed the CUDA runtime.\n",
    "ort_sess = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "for i in ort_sess.get_inputs():\n",
    "    print(f'input: {i.name}, {i.shape}, {i.type}')\n",
    "    \n",
    "print(f'Testing {count} rows')\n",
    "failed = 0\n",
    "for i in range(val_data.data.shape[0]):\n",
    "    data = val_data.data[i]    \n",
    "    expected = int(val_data.train_labels[i])\n",
    "\n",
    "    while len(data.shape) < 4:\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "    outputs = ort_sess.run(None, {'input': data.astype(np.float32) / 255.0})\n",
    "    result = outputs[0]\n",
    "    index = np.argmax(result)\n",
    "    label = val_data.classes[index]\n",
    "    if expected != index:\n",
    "        # print(f'### Failed: {expected} and got {label}')\n",
    "        failed += 1\n",
    "          \n",
    "rate = (count - failed) * 100 / count\n",
    "print(f\"Failed {failed} out of {count} rows\")\n",
    "print(f'Inference pass rate is  {rate} %.')\n",
    "print(f'How does this compare with the training validation accuracy of {top_accuracy}')\n",
    "if np.isclose(rate, top_accuracy* 100, atol=0.1):\n",
    "    print('Success! The model is working as expected.')\n",
    "else:\n",
    "    print('The onnx runtime is giving different results.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "799abcba35f70097d02fca042963180a03ec3451fe1b7671ac5d22383cd0232c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
