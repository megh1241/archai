{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Search\n",
    "\n",
    "This notebook and accompanying code shows how to run an Archai Network Architecture Search (NAS) using\n",
    "an Azure Machine Learning Workspace with distributed partial training of models on a GPU cluster.\n",
    "This example requires a storage account and Azure machine learning workspace specified in a config.json\n",
    "file like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"subscription_id\": \"...\",\n",
    "    \"resource_group\": \"...\",\n",
    "    \"workspace_name\": \"...\",\n",
    "    \"storage_account_key\": \"...\",\n",
    "    \"storage_account_name\": \"...\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import archai.common.azureml_helper as aml_helper\n",
    "from azure.ai.ml.entities import UserIdentityConfiguration\n",
    "\n",
    "sys.path.append(os.path.realpath('scripts'))\n",
    "\n",
    "# make sure we have a scripts dir for the code to run our jobs.\n",
    "import os\n",
    "scripts_dir = \"./scripts\"\n",
    "data_scripts_dir = \"./data_prep\"\n",
    "os.makedirs(scripts_dir, exist_ok=True)\n",
    "\n",
    "config_file = \"../.azureml/config.json\"\n",
    "config = json.load(open(config_file, 'r'))\n",
    "\n",
    "for required_key in ['subscription_id', 'resource_group', 'workspace_name', 'storage_account_key', 'storage_account_name']:\n",
    "    if not required_key in config:\n",
    "        print(f\"### Error: please add a {required_key} to {config_file}\")\n",
    "\n",
    "storage_account_key = config['storage_account_key']    \n",
    "storage_account_name = config['storage_account_name']\n",
    "\n",
    "print(f'Using storage account: {storage_account_name}')\n",
    "\n",
    "# This is the AML experiment name \n",
    "experiment_name = 'mnist_test_run'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our [search.py](scripts/search.py) will use a ConfigSearchSpace based on the following parameters, the model defined in [model.py](scripts/model.py) will take various configurations and build different shape CNN models for each configuration as shown below. Each time you execute this cell it will generate a new random CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from archai.discrete_search.search_spaces.config import ArchParamTree, DiscreteChoice, ArchConfig\n",
    "from model import MyModel\n",
    "\n",
    "arch_param_tree = ArchParamTree({\n",
    "    'nb_layers': DiscreteChoice(list(range(1, 13))),\n",
    "    'kernel_size': DiscreteChoice([1, 3, 5, 7]),\n",
    "    'hidden_dim': DiscreteChoice([16, 32, 64, 128])\n",
    "})\n",
    "\n",
    "arch_config = arch_param_tree.sample_config()\n",
    "print(arch_config)\n",
    "\n",
    "MyModel(arch_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a handle to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = aml_helper.get_aml_client_from_file(config_path=config_file)\n",
    "print(f'Using workspace \"{ml_client.workspace_name}\" in resource group \"{ml_client.resource_group_name}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the compute clusters that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cpu cluster for running the search\n",
    "cpu_compute_name = \"nas-cpu-cluster-D14-v2\"\n",
    "aml_helper.create_compute_cluster(ml_client, cpu_compute_name, size=\"Standard_D14_v2\", location=\"westus2\")\n",
    "\n",
    "# Create gpu cluster for running the search\n",
    "gpu_compute_name = \"nas-gpu-cluster-NC6\"\n",
    "aml_helper.create_compute_cluster(ml_client, gpu_compute_name, size=\"Standard_NC6\", location=\"westus2\", max_instances=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the AML Environment from our conda.yaml file.  This ensures our conda environment contains the Archai framework that we are using here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archai_job_env = aml_helper.create_environment_from_file(ml_client, conda_file=\"conda.yaml\", version='0.1.7')\n",
    "environment_name = f\"{archai_job_env.name}:{archai_job_env.version}\"\n",
    "print(environment_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure our storage account is setup with a `models` blob store container for storing the final onnx models, a `datasets` blob store for our training dataset and a `status` storage table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from store import ArchaiStore\n",
    "\n",
    "# Register the datastore with AML\n",
    "data_store_name = \"datasets\"\n",
    "data_container_name = \"datasets\"\n",
    "model_store_name = \"models\"\n",
    "model_container_name = \"models\"\n",
    "root_folder = experiment_name\n",
    "\n",
    "# make sure the datasets container exists\n",
    "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=data_container_name)\n",
    "store.upload_blob(root_folder, config_file)\n",
    "\n",
    "# make sure the models container exists\n",
    "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=model_container_name)\n",
    "store.upload_blob(\"config\", config_file)\n",
    "\n",
    "datastore_path = f'azureml://datastores/{data_store_name}/paths/{root_folder}'\n",
    "results_path = f'azureml://datastores/{model_store_name}/paths/{root_folder}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the `models` blob store container in the ML workspace `datastores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AzureBlobDatastore\n",
    "from azure.ai.ml.entities._credentials import AccountKeyConfiguration\n",
    "\n",
    "try:\n",
    "    model_store = ml_client.datastores.get(model_store_name)\n",
    "except:    \n",
    "    model_store = AzureBlobDatastore(\n",
    "        name=model_store_name,\n",
    "        description=\"Datastore pointing to our models blob container.\",\n",
    "        account_name=storage_account_name,\n",
    "        container_name=model_container_name,\n",
    "        credentials=AccountKeyConfiguration(\n",
    "            account_key=storage_account_key\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ml_client.create_or_update(model_store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the `datasets` blob store container in the ML workspace `datastores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data_store = ml_client.datastores.get(data_store_name)\n",
    "except:\n",
    "    data_store = AzureBlobDatastore(\n",
    "        name=data_store_name,\n",
    "        description=\"Datastore pointing to our dataset container.\",\n",
    "        account_name=storage_account_name,\n",
    "        container_name=data_container_name,\n",
    "        credentials=AccountKeyConfiguration(\n",
    "            account_key=storage_account_key\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ml_client.create_or_update(data_store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline command that prepares our training dataset using `prep_data_store.py`.  This pipeline will write the output do our `datasets` blob store so the training jobs can find the dataset there all ready to go.  That way each training job doesn't have to repeat the download and preparation of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_prep_component = command(\n",
    "    name=\"data_prep2\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Downloads the remote dataset to our blob store.\",\n",
    "    inputs= {\n",
    "        \"name\": Input(type='string')\n",
    "    },\n",
    "    outputs= {\n",
    "        \"data\": Output(type=\"uri_folder\", path=datastore_path, mode=\"rw_mount\")\n",
    "    },\n",
    "\n",
    "    # The source folder of the component\n",
    "    code=data_scripts_dir,\n",
    "    command=\"\"\"python3 prep_data_store.py \\\n",
    "            --path ${{outputs.data}} \\\n",
    "            \"\"\",\n",
    "    environment=environment_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a command that kicks off the Archai Search using `search.py`, it will take the dataset as input so it can pass it along to the training jobs later on.  It also produces some search output files, .png charts, and json results files which will also go into our `models` blob store under the folder named the same as our `experiment_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_config = bytes(json.dumps(config), encoding='utf-8').hex()\n",
    "\n",
    "partial_epochs = 0.1\n",
    "\n",
    "output_path = results_path + '/' + experiment_name\n",
    "fixed_args = f'--data_dir {datastore_path} ' + \\\n",
    "             f'--output_dir {results_path} ' + \\\n",
    "             f'--environment \"{environment_name}\" ' + \\\n",
    "             f'--experiment \"{experiment_name}\" ' + \\\n",
    "             f'--compute \"{gpu_compute_name}\" ' + \\\n",
    "             f'--config \"{hex_config}\" ' + \\\n",
    "             f'--partial_training_epochs {partial_epochs} '\n",
    "\n",
    "print(fixed_args)\n",
    "\n",
    "search_component = command(\n",
    "    name=\"search\",\n",
    "    display_name=\"The Archai NAS search\",\n",
    "    description=\"Runs the NAS search algorithm.\",    \n",
    "    inputs= {\n",
    "        \"data\": Input(type=\"uri_folder\")\n",
    "    },\n",
    "    outputs= {\n",
    "        \"results\": Output(type=\"uri_folder\", path=output_path, mode=\"rw_mount\")\n",
    "    },\n",
    "    code=scripts_dir,\n",
    "    identity= UserIdentityConfiguration(),\n",
    "    command='python3 search.py --local_output ${{outputs.results}} ' + \\\n",
    "        fixed_args,\n",
    "    environment=environment_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an AML pipeline with the data prep and search components piping the output of the data prep to the search component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import dsl, Input, Output\n",
    "from commands import make_dynamic_training_subgraph\n",
    "\n",
    "full_epochs = 10\n",
    "hex_config = bytes(json.dumps(config), encoding='utf-8').hex()\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_name,\n",
    "    description=\"Data prep pipeline\",\n",
    ")\n",
    "def mnist_search_pipeline():\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep_component(\n",
    "        name=\"MNIST\"\n",
    "    )\n",
    "\n",
    "    search_job = search_component(\n",
    "        data=data_prep_job.outputs.data\n",
    "    )\n",
    "    \n",
    "    subgraph_node = make_dynamic_training_subgraph(\n",
    "        results_path=results_path,\n",
    "        environment_name=environment_name,\n",
    "        storage_account_key=storage_account_key,\n",
    "        storage_account_name=storage_account_name,\n",
    "        subscription_id=ml_client.subscription_id,\n",
    "        resource_group_name=ml_client.resource_group_name,\n",
    "        workspace_name=ml_client.workspace_name,\n",
    "        hex_config=hex_config,\n",
    "        scripts_dir=scripts_dir,\n",
    "        full_epochs=full_epochs)(\n",
    "        top_models_folder=search_job.outputs.results,\n",
    "        data=data_prep_job.outputs.data\n",
    "    )\n",
    "    \n",
    "    # Note: this user identity is required to submit a dynamic run since we need to create the dynamic \n",
    "    # run on behalf of the user\n",
    "    subgraph_node.identity = UserIdentityConfiguration()\n",
    "    # Note: private preview flag is recommended to set if there are preview features inside dynamic pipeline\n",
    "    subgraph_node.environment_variables = {\"AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED\": \"true\"}    \n",
    "\n",
    "    return {\n",
    "        \"results\": subgraph_node.outputs.output\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = mnist_search_pipeline()\n",
    "\n",
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=experiment_name,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the pipeline azure ML studio portal in your web browser (This works when you are running this notbook in VS code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(pipeline_job.services[\"Studio\"].endpoint)\n",
    "\n",
    "job_name = pipeline_job.name\n",
    "print(f'Started pipeline: {job_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can fetch any pipeline job again if you needed to continue this notebook later:\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "# job_name = 'silly_peach_sg8c72n9mv'\n",
    "pipeline_job = ml_client.jobs.get(job_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Take a look at the pareto curve plots.  This cell can be run multiple times and you will see updates as each iteration finishes.\n",
    "You can even run this later after restarting the jupyter notebook because it is not dependent on variable state it is only\n",
    "dependent on the persistent 'models' blob store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results import get_results, show_results, download_best_models\n",
    "from store import ArchaiStore\n",
    "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=model_container_name)\n",
    "\n",
    "print(f'Fetching results for {experiment_name}...')\n",
    "blob_path = root_folder + '/' + experiment_name\n",
    "output_folder = experiment_name\n",
    "\n",
    "get_results(store, blob_path, output_folder)\n",
    "show_results(output_folder)\n",
    "download_best_models(store, experiment_name, output_folder)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ONNX Runtime Inference on the Best Model\n",
    "\n",
    "When the search pipeline completes you should have a `top_models.json` file in the experiment_name output folder and you can use that to find the most accurate model and run it through the ONNX runtime to see if the ONNX inference gets the same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top model in the json results.\n",
    "filename = os.path.join(output_folder, \"top_models.json\")\n",
    "best_of_the_best = None\n",
    "top_accuracy = 0\n",
    "row = None\n",
    "if not os.path.isfile(filename):\n",
    "    raise Exception(f\"Could not find {filename} file. Please wait for job to finish.\")\n",
    "\n",
    "results = json.load(open(filename, \"r\"))\n",
    "models = results['top_models']\n",
    "for a in models:\n",
    "    if type(a) is dict and 'val_acc' in a:\n",
    "        val_acc = a['val_acc']\n",
    "        if val_acc > top_accuracy:\n",
    "            top_accuracy = val_acc\n",
    "            best_of_the_best = a['job_id']\n",
    "            row = a\n",
    "\n",
    "config = ArchConfig(row)\n",
    "model = MyModel(config)\n",
    "\n",
    "arch = f\"nb_layers={model.nb_layers}, kernel_size={model.kernel_size}, hidden_dim={model.hidden_dim}\"\n",
    "print(f\"The top model is {best_of_the_best} with accuracy {top_accuracy} and architecture {arch}\")\n",
    "\n",
    "blob_path = root_folder + '/' + best_of_the_best\n",
    "model_output = os.path.join(output_folder, 'top_model')\n",
    "get_results(store, blob_path, model_output)\n",
    "\n",
    "model_path = os.path.join(model_output, 'model.onnx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great, now let's test if this model works as advertised.\n",
    "from archai.datasets.cv.mnist_dataset_provider import MnistDatasetProvider\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "dataset_provider = MnistDatasetProvider()\n",
    "val_data = dataset_provider.get_val_dataset()\n",
    "count = val_data.data.shape[0]\n",
    "test = np.random.choice(count, 1)[0]\n",
    "data = val_data.data[test]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# check what the images look like.\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(data, cmap='gray')\n",
    "print(f'data has shape: {data.shape}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Now run the ONNX runtime on this the validation set.\n",
    "# You can change this to `CUDAExecutionProvider` if you have a GPU and have\n",
    "# installed the CUDA runtime.\n",
    "ort_sess = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "for i in ort_sess.get_inputs():\n",
    "    print(f'input: {i.name}, {i.shape}, {i.type}')\n",
    "    \n",
    "print(f'Testing {count} rows')\n",
    "failed = 0\n",
    "for i in range(val_data.data.shape[0]):\n",
    "    data = val_data.data[i]    \n",
    "    expected = int(val_data.train_labels[i])\n",
    "\n",
    "    while len(data.shape) < 4:\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "    outputs = ort_sess.run(None, {'input': data.astype(np.float32) / 255.0})\n",
    "    result = outputs[0]\n",
    "    index = np.argmax(result)\n",
    "    label = val_data.classes[index]\n",
    "    if expected != index:\n",
    "        # print(f'### Failed: {expected} and got {label}')\n",
    "        failed += 1\n",
    "          \n",
    "rate = (count - failed) * 100 / count\n",
    "print(f\"Failed {failed} out of {count} rows\")\n",
    "print(f'Inference pass rate is  {rate} %.')\n",
    "print(f'How does this compare with the training validation accuracy of {top_accuracy}')\n",
    "if np.isclose(rate, top_accuracy* 100, atol=0.1):\n",
    "    print('Success! The model is working as expected.')\n",
    "else:\n",
    "    print('The onnx runtime is giving different results.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "799abcba35f70097d02fca042963180a03ec3451fe1b7671ac5d22383cd0232c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
