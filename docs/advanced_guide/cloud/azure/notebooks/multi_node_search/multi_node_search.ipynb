{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-node Search\n",
        "\n",
        "This notebook and accompanying code shows how to run an\n",
        "[Archai](https://github.com/microsoft/archai/) Neural Architecture Search (NAS) using an [Azure\n",
        "Machine Learning Workspace](https://ml.azure.com/) with partial training of models (on a GPU\n",
        "cluster) providing validation accuracies to guide that search. This notebook requires that you have\n",
        "the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed and logged\n",
        "in otherwise the create compute cluster cell will fail.  You will also need to create an Azure ML\n",
        "workspace using [https://ml.azure.com] and an Azure storage account.  The storage account does not\n",
        "need to be in the same resource group as the workspace.\n",
        "\n",
        "This notebook also assumes you have a python environment setup using:\n",
        "\n",
        "```\n",
        "pip install -e .[aml] --extra-index-url https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n",
        "```\n",
        "in your Archai repository root. This example requires a `config.json` file containing the information about your\n",
        "Azure subscription, the Azure ML workspace name and resource group, and the azure storage account key and name:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"subscription_id\": \"...\",\n",
        "    \"resource_group\": \"...\",\n",
        "    \"workspace_name\": \"...\",\n",
        "    \"storage_account_key\": \"...\",\n",
        "    \"storage_account_name\": \"...\"\n",
        "}\n",
        "```\n",
        "See:\n",
        "- [Set up a Python development environment for Azure Machine Learning](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#local-and-dsvm-only-create-a-workspace-configuration-file) \n",
        "- [Get your Storage Account keys](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using workspace archai-aml-test and storage account: archaimnistmodels\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import archai.common.azureml_helper as aml_helper\n",
        "import archai.common.notebook_helper as nb_helper\n",
        "from azure.ai.ml.entities import UserIdentityConfiguration\n",
        "\n",
        "# locate the code that we will use in this notebook.\n",
        "scripts_dir = \"./scripts\"\n",
        "data_scripts_dir = \"./data_prep\"\n",
        "\n",
        "config_file = \"../.azureml/config.json\"\n",
        "config = json.load(open(config_file, 'r'))\n",
        "\n",
        "for required_key in ['subscription_id', 'resource_group', 'workspace_name', 'storage_account_key', 'storage_account_name']:\n",
        "    if not required_key in config:\n",
        "        print(f\"### Error: please add a {required_key} to {config_file}\")\n",
        "\n",
        "storage_account_key = config['storage_account_key']    \n",
        "storage_account_name = config['storage_account_name']\n",
        "workspace_name = config['workspace_name']\n",
        "\n",
        "print(f'Using workspace {workspace_name} and storage account: {storage_account_name}')\n",
        "\n",
        "# This is the AML experiment name \n",
        "experiment_name = 'mnist_test_run'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our [search.py](scripts/search.py) will use a ConfigSearchSpace based on the following parameters, the model defined in [model.py](scripts/model.py) will take various configurations and build different shape CNN models for each configuration as shown below. Each time you execute this cell it will generate a new random CNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ArchConfig({\n",
            "    \"nb_layers\": 5,\n",
            "    \"kernel_size\": 7,\n",
            "    \"hidden_dim\": 16\n",
            "})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (cross_entropy_loss): CrossEntropyLoss()\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): ReLU()\n",
              "    (9): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU()\n",
              "    (15): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (16): Conv2d(16, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from archai.discrete_search.search_spaces.config import ArchParamTree, DiscreteChoice\n",
        "from scripts.model import MyModel\n",
        "\n",
        "arch_param_tree = ArchParamTree({\n",
        "    'nb_layers': DiscreteChoice(list(range(1, 13))),\n",
        "    'kernel_size': DiscreteChoice([1, 3, 5, 7]),\n",
        "    'hidden_dim': DiscreteChoice([16, 32, 64, 128])\n",
        "})\n",
        "\n",
        "arch_config = arch_param_tree.sample_config()\n",
        "print(arch_config)\n",
        "\n",
        "MyModel(arch_config)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get a handle to the workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: ..\\.azureml\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using workspace \"archai-aml-test\" in resource group \"snpe-aml-rg\"\n"
          ]
        }
      ],
      "source": [
        "ml_client = aml_helper.get_aml_client_from_file(config_path=config_file)\n",
        "print(f'Using workspace \"{ml_client.workspace_name}\" in resource group \"{ml_client.resource_group_name}\"')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the compute clusters that we need.  If this cell fails with `DefaultAzureCredential failed to retrieve a token from the included credentials`.\n",
        "then you might need to run `az login` from the command line using the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).\n",
        "If it also fails with `ResourceNotFoundError: (ParentResourceNotFound)` then you may need to run `az account set --subscription ...` with the \n",
        "subscription id you specified in the above `config.json` file and check that the resource group you specified really does contain the Azure ML workspace\n",
        "you specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You already have a cluster named nas-cpu-cluster-D14-v2, we'll reuse it as is.\n",
            "You already have a cluster named nas-gpu-cluster-NC6, we'll reuse it as is.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AmlCompute({'type': 'amlcompute', 'created_on': None, 'provisioning_state': 'Succeeded', 'provisioning_errors': None, 'name': 'nas-gpu-cluster-NC6', 'description': None, 'tags': None, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/c8b7f913-60fb-4759-a310-fc5630e56f99/resourceGroups/snpe-aml-rg/providers/Microsoft.MachineLearningServices/workspaces/archai-aml-test/computes/nas-gpu-cluster-NC6', 'Resource__source_path': None, 'base_path': 'd:\\\\git\\\\microsoft\\\\archai\\\\archai\\\\docs\\\\advanced_guide\\\\cloud\\\\azure\\\\notebooks\\\\multi_node_search', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000021C474BD600>, 'resource_id': None, 'location': 'westus2', 'size': 'STANDARD_NC6', 'min_instances': 0, 'max_instances': 8, 'idle_time_before_scale_down': 180.0, 'identity': None, 'ssh_public_access_enabled': True, 'ssh_settings': None, 'network_settings': <azure.ai.ml.entities._compute.compute.NetworkSettings object at 0x0000021C67692620>, 'tier': 'dedicated', 'enable_node_public_ip': True, 'subnet': None})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create cpu cluster for running the search\n",
        "cpu_compute_name = \"nas-cpu-cluster-D14-v2\"\n",
        "aml_helper.create_compute_cluster(ml_client, cpu_compute_name, size=\"Standard_D14_v2\", location=\"westus2\")\n",
        "\n",
        "# Create gpu cluster for running the search\n",
        "gpu_compute_name = \"nas-gpu-cluster-NC6\"\n",
        "aml_helper.create_compute_cluster(ml_client, gpu_compute_name, size=\"Standard_NC6\", location=\"westus2\", max_instances=8)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the AML Environment from our conda.yaml file.  This ensures our conda environment contains the Archai framework that we are using here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment with name aml-archai is registered to workspace, the environment version is 0.1.12\n",
            "aml-archai:0.1.12\n"
          ]
        }
      ],
      "source": [
        "archai_job_env = aml_helper.create_environment_from_file(ml_client, \n",
        "                                                         image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
        "                                                         conda_file=\"conda.yaml\", \n",
        "                                                         version='0.1.12')\n",
        "environment_name = f\"{archai_job_env.name}:{archai_job_env.version}\"\n",
        "print(environment_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensure our storage account is setup with a `models` blob store container for storing the final onnx models, a `datasets` blob store for our training dataset and a `status` storage table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "from archai.common.store import ArchaiStore\n",
        "\n",
        "# Register the datastore with AML\n",
        "data_store_name = \"datasets\"\n",
        "data_container_name = \"datasets\"\n",
        "model_store_name = \"models\"\n",
        "model_container_name = \"models\"\n",
        "root_folder = experiment_name\n",
        "\n",
        "# make sure the datasets container exists\n",
        "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=data_container_name)\n",
        "store.upload_blob(root_folder, config_file)\n",
        "\n",
        "# make sure the models container exists\n",
        "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=model_container_name)\n",
        "store.upload_blob(\"config\", config_file)\n",
        "\n",
        "datastore_path = f'azureml://datastores/{data_store_name}/paths/{root_folder}'\n",
        "results_path = f'azureml://datastores/{model_store_name}/paths/{root_folder}'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Register the `models` blob store container in the ML workspace `datastores`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AzureBlobDatastore\n",
        "from azure.ai.ml.entities._credentials import AccountKeyConfiguration\n",
        "\n",
        "try:\n",
        "    model_store = ml_client.datastores.get(model_store_name)\n",
        "except:    \n",
        "    model_store = AzureBlobDatastore(\n",
        "        name=model_store_name,\n",
        "        description=\"Datastore pointing to our models blob container.\",\n",
        "        account_name=storage_account_name,\n",
        "        container_name=model_container_name,\n",
        "        credentials=AccountKeyConfiguration(\n",
        "            account_key=storage_account_key\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    ml_client.create_or_update(model_store)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Register the `datasets` blob store container in the ML workspace `datastores`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    data_store = ml_client.datastores.get(data_store_name)\n",
        "except:\n",
        "    data_store = AzureBlobDatastore(\n",
        "        name=data_store_name,\n",
        "        description=\"Datastore pointing to our dataset container.\",\n",
        "        account_name=storage_account_name,\n",
        "        container_name=data_container_name,\n",
        "        credentials=AccountKeyConfiguration(\n",
        "            account_key=storage_account_key\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    ml_client.create_or_update(data_store)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a pipeline command that prepares our training dataset using `prep_data_store.py`.  This pipeline will write the output do our `datasets` blob store so the training jobs can find the dataset there all ready to go.  That way each training job doesn't have to repeat the download and preparation of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input, Output\n",
        "\n",
        "data_prep_component = command(\n",
        "    name=\"data_prep2\",\n",
        "    display_name=\"Data preparation for training\",\n",
        "    description=\"Downloads the remote dataset to our blob store.\",  \n",
        "    inputs= {\n",
        "        \"name\": Input(type='string')\n",
        "    },\n",
        "    outputs= {\n",
        "        \"data\": Output(type=\"uri_folder\", path=datastore_path, mode=\"rw_mount\")\n",
        "    },\n",
        "\n",
        "    # The source folder of the component\n",
        "    code=data_scripts_dir,\n",
        "    command=\"\"\"python3 prep_data_store.py \\\n",
        "            --path ${{outputs.data}} \\\n",
        "            \"\"\",\n",
        "    environment=environment_name,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a command that kicks off the Archai Search using `search.py`, it will take the dataset as input so it can pass it along to the training jobs later on.  It also produces some search output files, .png charts, and json results files which will also go into our `models` blob store under the folder named the same as our `experiment_name`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "hex_config = bytes(json.dumps(config), encoding='utf-8').hex()\n",
        "\n",
        "partial_epochs = 0.1\n",
        "max_unseen_population = 16  # best if this is an even multiple our gpu cluster size, we'll get much better throughput.\n",
        "search_iterations = 5  # for quick debugging.\n",
        "init_num_models = 10\n",
        "\n",
        "output_path = results_path + '/' + experiment_name\n",
        "\n",
        "search_component = command(\n",
        "    name=\"search\",\n",
        "    display_name=\"The Archai NAS search\",\n",
        "    description=\"Runs the NAS search algorithm.\",    \n",
        "    is_deterministic=False,\n",
        "    inputs= {\n",
        "        \"data\": Input(type=\"uri_folder\"),\n",
        "        \"data_dir\": Input(type=\"string\", default=datastore_path),\n",
        "        \"output_dir\": Input(type=\"string\", default=output_path),\n",
        "        \"environment_name\": Input(type=\"string\", default=environment_name),\n",
        "        \"experiment\": Input(type=\"string\", default=experiment_name),\n",
        "        \"compute\": Input(type=\"string\", default=gpu_compute_name),\n",
        "        \"config\": Input(type=\"string\", default=hex_config),\n",
        "        \"search_iterations\": Input(type=\"integer\", default=search_iterations),\n",
        "        \"init_num_models\":  Input(type=\"integer\", default=init_num_models),\n",
        "        \"max_unseen_population\":  Input(type=\"integer\", default=max_unseen_population),\n",
        "        \"partial_training_epochs\":  Input(type=\"integer\", default=partial_epochs)\n",
        "    },\n",
        "    outputs= {\n",
        "        \"results\": Output(type=\"uri_folder\", path=output_path, mode=\"rw_mount\")\n",
        "    },\n",
        "    code=scripts_dir,\n",
        "    identity= UserIdentityConfiguration(),\n",
        "    command=\"\"\"python3 search.py --data_dir ${{inputs.data_dir}} \\\n",
        "                                --output_dir ${{inputs.output_dir}} \\\n",
        "                                --environment ${{inputs.environment_name}} \\\n",
        "                                --experiment ${{inputs.experiment}} \\\n",
        "                                --compute ${{inputs.compute}} \\\n",
        "                                --config ${{inputs.config}} \\\n",
        "                                --search_iterations ${{inputs.search_iterations}} \\\n",
        "                                --init_num_models ${{inputs.init_num_models}} \\\n",
        "                                --max_unseen_population ${{inputs.max_unseen_population}} \\\n",
        "                                --partial_training_epochs ${{inputs.partial_training_epochs}} \\\n",
        "                                --local_output ${{outputs.results}} \\                                \n",
        "            \"\"\",\n",
        "    environment=environment_name,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This creates a command that will do full training on the final list of the best models produced by the above search command. Also creates a monitor command that monitors all the parallel training jobs and gathers the results when they are all complete updating our final `models.json` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "from scripts.commands import make_training_pipeline_command, make_monitor_command\n",
        "full_epochs = 10\n",
        "timeout = 3600\n",
        "hex_config = bytes(json.dumps(config), encoding='utf-8').hex()\n",
        "\n",
        "full_training_component = make_training_pipeline_command(\n",
        "    \"Full Training Pipeline\", hex_config, scripts_dir, gpu_compute_name, \n",
        "    datastore_path, output_path, experiment_name, environment_name, full_epochs, save_models=True)\n",
        "                                                 \n",
        "monitor_component = make_monitor_command(hex_config, scripts_dir, results_path, environment_name, timeout)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create an AML pipeline that pipes the output of the data prep to the search component, then when search is finished, starts a full training job of the top models and then waits for all that training to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import dsl\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=cpu_compute_name,\n",
        "    description=\"Archai search pipeline\",\n",
        ")\n",
        "def archai_search_pipeline():\n",
        "    \n",
        "    data_prep_job = data_prep_component(\n",
        "        name=\"MNIST\"\n",
        "    )\n",
        "\n",
        "    search_job = search_component(\n",
        "        data=data_prep_job.outputs.data\n",
        "    )\n",
        "    \n",
        "    training_job = full_training_component(\n",
        "        models=search_job.outputs.results,\n",
        "        data=data_prep_job.outputs.data\n",
        "    )\n",
        "\n",
        "    monitor_job = monitor_component(\n",
        "        models=search_job.outputs.results,\n",
        "        training_results=training_job.outputs.results\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"results\": monitor_job.outputs.results\n",
        "    }\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Submit the pipeline job so it starts running in your Azure ML workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [
        {
          "ename": "HttpResponseError",
          "evalue": "(UserError) Error occurred when loading YAML file rootNode, details: Command \"python3 search.py --data_dir ${{inputs.data_dir}}                                 --output_dir ${{inputs.output_dir}}                                 --environment ${{inputs.environment_name}}                                 --experiment ${{inputs.experiment}}                                 --compute ${{inputs.compute}}                                 --config ${{inputs.config}}                                 --search_iterations ${{inputs.search_iterations}}                                 --init_num_models ${{inputs.init_num_models}}                                 --max_unseen_population ${{inputs.max_unseen_population}}                                 --partial_training_epochs ${{inputs.partial_training_epochs}}                                 --local_output ${{outputs.results}} \\                                \n            \" has error:\r\nBackslash-newline is not supported.\nCode: UserError\nMessage: Error occurred when loading YAML file rootNode, details: Command \"python3 search.py --data_dir ${{inputs.data_dir}}                                 --output_dir ${{inputs.output_dir}}                                 --environment ${{inputs.environment_name}}                                 --experiment ${{inputs.experiment}}                                 --compute ${{inputs.compute}}                                 --config ${{inputs.config}}                                 --search_iterations ${{inputs.search_iterations}}                                 --init_num_models ${{inputs.init_num_models}}                                 --max_unseen_population ${{inputs.max_unseen_population}}                                 --partial_training_epochs ${{inputs.partial_training_epochs}}                                 --local_output ${{outputs.results}} \\                                \n            \" has error:\r\nBackslash-newline is not supported.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"2703a78633006b0803d135fe17a6779f\",\n        \"request\": \"a55494e9f07779d1\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"westus2\"\n}Type: Location\nInfo: {\n    \"value\": \"westus2\"\n}Type: Time\nInfo: {\n    \"value\": \"2023-03-24T02:11:30.439138+00:00\"\n}Type: InnerError\nInfo: {\n    \"value\": {\n        \"code\": \"BadArgument\",\n        \"innerError\": {\n            \"code\": \"ArgumentInvalid\",\n            \"innerError\": {\n                \"code\": \"YamlFileInvalid\",\n                \"innerError\": null\n            }\n        }\n    }\n}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipeline_job \u001b[39m=\u001b[39m ml_client\u001b[39m.\u001b[39;49mjobs\u001b[39m.\u001b[39;49mcreate_or_update(\n\u001b[0;32m      2\u001b[0m     archai_search_pipeline(),\n\u001b[0;32m      3\u001b[0m     \u001b[39m# Project's name\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m     experiment_name\u001b[39m=\u001b[39;49mexperiment_name,\n\u001b[0;32m      5\u001b[0m )\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:337\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m dimensions \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparameter_dimensions, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(custom_dimensions \u001b[39mor\u001b[39;00m {})}\n\u001b[0;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m log_activity(logger, activity_name \u001b[39mor\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, dimensions) \u001b[39mas\u001b[39;00m activityLogger:\n\u001b[1;32m--> 337\u001b[0m     return_value \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parameter_dimensions:\n\u001b[0;32m    339\u001b[0m         \u001b[39m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         activityLogger\u001b[39m.\u001b[39mactivity_info\u001b[39m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:595\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[1;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m     log_and_raise_error(ex)\n\u001b[0;32m    594\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 595\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:529\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[1;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate(job, raise_on_failure\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    528\u001b[0m \u001b[39m# Create all dependent resources\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_arm_id_or_upload_dependencies(job)\n\u001b[0;32m    531\u001b[0m git_props \u001b[39m=\u001b[39m get_git_properties()\n\u001b[0;32m    532\u001b[0m \u001b[39m# Do not add git props if they already exist in job properties.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39m# This is for update specifically-- if the user switches branches and tries to update\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[39m# their job, the request will fail since the git props will be repopulated.\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39m# MFE does not allow existing properties to be updated, only for new props to be added\u001b[39;00m\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:876\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_upload_dependencies\u001b[1;34m(self, job)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_resolve_arm_id_or_upload_dependencies\u001b[39m(\u001b[39mself\u001b[39m, job: Job) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This method converts name or name:version to ARM id. Or it\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[39m    registers/uploads nested dependencies.\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[39m    :rtype: Job\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_arm_id_or_azureml_id(job, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orchestrators\u001b[39m.\u001b[39;49mget_asset_arm_id)\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(job, PipelineJob):\n\u001b[0;32m    879\u001b[0m         \u001b[39m# Resolve top-level inputs\u001b[39;00m\n\u001b[0;32m    880\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_pipeline_job_inputs(job, job\u001b[39m.\u001b[39m_base_path)\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:1101\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_or_azureml_id\u001b[1;34m(self, job, resolver)\u001b[0m\n\u001b[0;32m   1099\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_arm_id_for_automl_job(job, resolver, inside_pipeline\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1100\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(job, PipelineJob):\n\u001b[1;32m-> 1101\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_arm_id_for_pipeline_job(job, resolver)\n\u001b[0;32m   1102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNon supported job type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(job)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:1216\u001b[0m, in \u001b[0;36mJobOperations._resolve_arm_id_for_pipeline_job\u001b[1;34m(self, pipeline_job, resolver)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# Process each component job\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1216\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_component_operations\u001b[39m.\u001b[39;49m_resolve_dependencies_for_pipeline_component_jobs(\n\u001b[0;32m   1217\u001b[0m         pipeline_job\u001b[39m.\u001b[39;49mcomponent, resolver\n\u001b[0;32m   1218\u001b[0m     )\n\u001b[0;32m   1219\u001b[0m \u001b[39mexcept\u001b[39;00m ComponentException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1220\u001b[0m     \u001b[39mraise\u001b[39;00m JobException(\n\u001b[0;32m   1221\u001b[0m         message\u001b[39m=\u001b[39me\u001b[39m.\u001b[39mmessage,\n\u001b[0;32m   1222\u001b[0m         target\u001b[39m=\u001b[39mErrorTarget\u001b[39m.\u001b[39mJOB,\n\u001b[0;32m   1223\u001b[0m         no_personal_data_message\u001b[39m=\u001b[39me\u001b[39m.\u001b[39mno_personal_data_message,\n\u001b[0;32m   1224\u001b[0m         error_category\u001b[39m=\u001b[39me\u001b[39m.\u001b[39merror_category,\n\u001b[0;32m   1225\u001b[0m     )\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_component_operations.py:754\u001b[0m, in \u001b[0;36mComponentOperations._resolve_dependencies_for_pipeline_component_jobs\u001b[1;34m(self, component, resolver, resolve_inputs)\u001b[0m\n\u001b[0;32m    746\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNon supported job type in Pipeline: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(job_instance)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    747\u001b[0m         \u001b[39mraise\u001b[39;00m ComponentException(\n\u001b[0;32m    748\u001b[0m             message\u001b[39m=\u001b[39mmsg,\n\u001b[0;32m    749\u001b[0m             target\u001b[39m=\u001b[39mErrorTarget\u001b[39m.\u001b[39mCOMPONENT,\n\u001b[0;32m    750\u001b[0m             no_personal_data_message\u001b[39m=\u001b[39mmsg,\n\u001b[0;32m    751\u001b[0m             error_category\u001b[39m=\u001b[39mErrorCategory\u001b[39m.\u001b[39mUSER_ERROR,\n\u001b[0;32m    752\u001b[0m         )\n\u001b[1;32m--> 754\u001b[0m component_cache\u001b[39m.\u001b[39;49mresolve_nodes()\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\_utils\\_cache_utils.py:377\u001b[0m, in \u001b[0;36mCachedNodeResolver.resolve_nodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39macquire()\n\u001b[0;32m    376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_nodes()\n\u001b[0;32m    378\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    379\u001b[0m     \u001b[39m# release lock even if exception happens\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\_utils\\_cache_utils.py:341\u001b[0m, in \u001b[0;36mCachedNodeResolver._resolve_nodes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m is_on_disk_cache_enabled() \u001b[39mand\u001b[39;00m is_private_preview_enabled():\n\u001b[0;32m    339\u001b[0m     cache_contents_to_resolve \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_cache_contents_from_disk(cache_contents_to_resolve)\n\u001b[1;32m--> 341\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolve_cache_contents(cache_contents_to_resolve, resolver\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resolver)\n\u001b[0;32m    343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_back_component_to_nodes(dict_of_nodes_to_resolve)\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\_utils\\_cache_utils.py:275\u001b[0m, in \u001b[0;36mCachedNodeResolver._resolve_cache_contents\u001b[1;34m(self, cache_contents_to_resolve, resolver)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m     resolution_results \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(_map_func, _components)\n\u001b[1;32m--> 275\u001b[0m \u001b[39mfor\u001b[39;00m cache_content, resolution_results \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(cache_contents_to_resolve, resolution_results):\n\u001b[0;32m    276\u001b[0m     cache_content\u001b[39m.\u001b[39marm_id \u001b[39m=\u001b[39m resolution_results\n\u001b[0;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m is_on_disk_cache_enabled() \u001b[39mand\u001b[39;00m is_private_preview_enabled():\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_operation_orchestrator.py:226\u001b[0m, in \u001b[0;36mOperationOrchestrator.get_asset_arm_id\u001b[1;34m(self, asset, azureml_type, register_asset, sub_workspace_resource)\u001b[0m\n\u001b[0;32m    224\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_data_arm_id(asset, register_asset\u001b[39m=\u001b[39mregister_asset)\n\u001b[0;32m    225\u001b[0m \u001b[39melif\u001b[39;00m azureml_type \u001b[39m==\u001b[39m AzureMLResourceType\u001b[39m.\u001b[39mCOMPONENT \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(asset, Component):\n\u001b[1;32m--> 226\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_component_arm_id(asset)\n\u001b[0;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUnsupported azureml type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for asset: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_operation_orchestrator.py:351\u001b[0m, in \u001b[0;36mOperationOrchestrator._get_component_arm_id\u001b[1;34m(self, component)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"If component arm id is already resolved, return the id Or get arm id via remote call, register the component\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[39mif necessary, and FILL BACK the arm id to component to reduce remote call.\"\"\"\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m component\u001b[39m.\u001b[39mid:\n\u001b[1;32m--> 351\u001b[0m     component\u001b[39m.\u001b[39m_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_component\u001b[39m.\u001b[39;49mcreate_or_update(\n\u001b[0;32m    352\u001b[0m         component, is_anonymous\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, show_progress\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_operation_config\u001b[39m.\u001b[39;49mshow_progress\n\u001b[0;32m    353\u001b[0m     )\u001b[39m.\u001b[39mid\n\u001b[0;32m    354\u001b[0m \u001b[39mreturn\u001b[39;00m component\u001b[39m.\u001b[39mid\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:337\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m dimensions \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparameter_dimensions, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(custom_dimensions \u001b[39mor\u001b[39;00m {})}\n\u001b[0;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m log_activity(logger, activity_name \u001b[39mor\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, dimensions) \u001b[39mas\u001b[39;00m activityLogger:\n\u001b[1;32m--> 337\u001b[0m     return_value \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parameter_dimensions:\n\u001b[0;32m    339\u001b[0m         \u001b[39m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         activityLogger\u001b[39m.\u001b[39mactivity_info\u001b[39m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_component_operations.py:372\u001b[0m, in \u001b[0;36mComponentOperations.create_or_update\u001b[1;34m(self, component, version, skip_validation, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m             result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_version_operation\u001b[39m.\u001b[39mcreate_or_update(\n\u001b[0;32m    364\u001b[0m                 name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    365\u001b[0m                 version\u001b[39m=\u001b[39mversion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_args,\n\u001b[0;32m    370\u001b[0m             )\n\u001b[0;32m    371\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 372\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[0;32m    375\u001b[0m     component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(name\u001b[39m=\u001b[39mcomponent\u001b[39m.\u001b[39mname, version\u001b[39m=\u001b[39mcomponent\u001b[39m.\u001b[39mversion)\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\operations\\_component_operations.py:363\u001b[0m, in \u001b[0;36mComponentOperations.create_or_update\u001b[1;34m(self, component, version, skip_validation, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m             result \u001b[39m=\u001b[39m _create_or_update_autoincrement(\n\u001b[0;32m    354\u001b[0m                 name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    355\u001b[0m                 body\u001b[39m=\u001b[39mrest_component_resource,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_args,\n\u001b[0;32m    361\u001b[0m             )\n\u001b[0;32m    362\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m             result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_version_operation\u001b[39m.\u001b[39mcreate_or_update(\n\u001b[0;32m    364\u001b[0m                 name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    365\u001b[0m                 version\u001b[39m=\u001b[39mversion,\n\u001b[0;32m    366\u001b[0m                 resource_group_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resource_group_name,\n\u001b[0;32m    367\u001b[0m                 workspace_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workspace_name,\n\u001b[0;32m    368\u001b[0m                 body\u001b[39m=\u001b[39mrest_component_resource,\n\u001b[0;32m    369\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_args,\n\u001b[0;32m    370\u001b[0m             )\n\u001b[0;32m    371\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    372\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\aml\\lib\\site-packages\\azure\\ai\\ml\\_restclient\\v2022_10_01\\operations\\_component_versions_operations.py:546\u001b[0m, in \u001b[0;36mComponentVersionsOperations.create_or_update\u001b[1;34m(self, resource_group_name, workspace_name, name, version, body, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m     map_error(status_code\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code, response\u001b[39m=\u001b[39mresponse, error_map\u001b[39m=\u001b[39merror_map)\n\u001b[0;32m    545\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize\u001b[39m.\u001b[39mfailsafe_deserialize(_models\u001b[39m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[1;32m--> 546\u001b[0m     \u001b[39mraise\u001b[39;00m HttpResponseError(response\u001b[39m=\u001b[39mresponse, model\u001b[39m=\u001b[39merror, error_format\u001b[39m=\u001b[39mARMErrorFormat)\n\u001b[0;32m    548\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m    549\u001b[0m     deserialized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deserialize(\u001b[39m'\u001b[39m\u001b[39mComponentVersion\u001b[39m\u001b[39m'\u001b[39m, pipeline_response)\n",
            "\u001b[1;31mHttpResponseError\u001b[0m: (UserError) Error occurred when loading YAML file rootNode, details: Command \"python3 search.py --data_dir ${{inputs.data_dir}}                                 --output_dir ${{inputs.output_dir}}                                 --environment ${{inputs.environment_name}}                                 --experiment ${{inputs.experiment}}                                 --compute ${{inputs.compute}}                                 --config ${{inputs.config}}                                 --search_iterations ${{inputs.search_iterations}}                                 --init_num_models ${{inputs.init_num_models}}                                 --max_unseen_population ${{inputs.max_unseen_population}}                                 --partial_training_epochs ${{inputs.partial_training_epochs}}                                 --local_output ${{outputs.results}} \\                                \n            \" has error:\r\nBackslash-newline is not supported.\nCode: UserError\nMessage: Error occurred when loading YAML file rootNode, details: Command \"python3 search.py --data_dir ${{inputs.data_dir}}                                 --output_dir ${{inputs.output_dir}}                                 --environment ${{inputs.environment_name}}                                 --experiment ${{inputs.experiment}}                                 --compute ${{inputs.compute}}                                 --config ${{inputs.config}}                                 --search_iterations ${{inputs.search_iterations}}                                 --init_num_models ${{inputs.init_num_models}}                                 --max_unseen_population ${{inputs.max_unseen_population}}                                 --partial_training_epochs ${{inputs.partial_training_epochs}}                                 --local_output ${{outputs.results}} \\                                \n            \" has error:\r\nBackslash-newline is not supported.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"2703a78633006b0803d135fe17a6779f\",\n        \"request\": \"a55494e9f07779d1\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"westus2\"\n}Type: Location\nInfo: {\n    \"value\": \"westus2\"\n}Type: Time\nInfo: {\n    \"value\": \"2023-03-24T02:11:30.439138+00:00\"\n}Type: InnerError\nInfo: {\n    \"value\": {\n        \"code\": \"BadArgument\",\n        \"innerError\": {\n            \"code\": \"ArgumentInvalid\",\n            \"innerError\": {\n                \"code\": \"YamlFileInvalid\",\n                \"innerError\": null\n            }\n        }\n    }\n}"
          ]
        }
      ],
      "source": [
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    archai_search_pipeline(),\n",
        "    # Project's name\n",
        "    experiment_name=experiment_name,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Open the pipeline azure ML studio portal in your web browser (this works when you are running this notebook in VS code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "import webbrowser\n",
        "webbrowser.open(pipeline_job.services[\"Studio\"].endpoint)\n",
        "\n",
        "job_name = pipeline_job.name\n",
        "print(f'Started pipeline: {job_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "# you can fetch any pipeline job again if you needed to continue this notebook later:\n",
        "from azure.ai.ml import dsl, Input, Output\n",
        "# job_name = 'icy_bear_p2d8pf4y84'\n",
        "pipeline_job = ml_client.jobs.get(job_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n",
        "\n",
        "Take a look at the pareto curve plots.  This cell can be run multiple times and you will see updates as each iteration finishes.\n",
        "You can even run this later after restarting the jupyter notebook because it is not dependent on variable state it is only\n",
        "dependent on the persistent 'models' blob store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "from scripts.utils import get_results, show_results, download_best_models\n",
        "from archai.common.store import ArchaiStore\n",
        "store = ArchaiStore(storage_account_name, storage_account_key, blob_container_name=model_container_name)\n",
        "\n",
        "print(f'Fetching results for {experiment_name}...')\n",
        "blob_path = root_folder + '/' + experiment_name\n",
        "output_folder = experiment_name\n",
        "\n",
        "get_results(store, blob_path, output_folder)\n",
        "download_best_models(store, experiment_name, output_folder)\n",
        "show_results(output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "df = nb_helper.get_search_csv(output_folder)\n",
        "df = df.drop('parent', axis=1)\n",
        "df = df.drop('parents', axis=1)\n",
        "\n",
        "csv_as_html = nb_helper.get_csv_as_stylized_html(df)\n",
        "display(HTML(csv_as_html))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test ONNX Runtime Inference on the Best Model\n",
        "\n",
        "When the search pipeline completes you should have a `models.json` file in the experiment_name output folder and you can use that to find the most accurate model and run it through the ONNX runtime to see if the ONNX inference gets the same accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "# find the top model in the json results.\n",
        "filename = os.path.join(output_folder, \"models.json\")\n",
        "best_of_the_best = None\n",
        "top_accuracy = -1\n",
        "row = None\n",
        "if not os.path.isfile(filename):\n",
        "    raise Exception(f\"Could not find {filename} file. Please wait for job to finish.\")\n",
        "\n",
        "results = json.load(open(filename, \"r\"))\n",
        "models = results['models']\n",
        "for a in models:\n",
        "    if type(a) is dict and 'val_acc' in a:\n",
        "        val_acc = a['val_acc']\n",
        "        if val_acc > top_accuracy:\n",
        "            top_accuracy = val_acc\n",
        "            best_of_the_best = a['id']\n",
        "            row = a\n",
        "\n",
        "model = MyModel(ArchConfig(row))\n",
        "\n",
        "arch = f\"nb_layers={model.nb_layers}, kernel_size={model.kernel_size}, hidden_dim={model.hidden_dim}\"\n",
        "print(f\"The top model is {best_of_the_best} with accuracy {top_accuracy} and architecture {arch}\")\n",
        "\n",
        "blob_path = root_folder + '/' + best_of_the_best\n",
        "model_output = os.path.join(output_folder, 'top_model')\n",
        "get_results(store, blob_path, model_output)\n",
        "\n",
        "model_path = os.path.join(model_output, 'model.onnx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": [
        "# Great, now let's test if this model works as advertised.\n",
        "from archai.datasets.cv.mnist_dataset_provider import MnistDatasetProvider\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "dataset_provider = MnistDatasetProvider()\n",
        "val_data = dataset_provider.get_val_dataset()\n",
        "count = val_data.data.shape[0]\n",
        "test = np.random.choice(count, 1)[0]\n",
        "data = val_data.data[test]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# check what the images look like.\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(data, cmap='gray')\n",
        "print(f'data has shape: {data.shape}')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Now run the ONNX runtime on this the validation set.\n",
        "# You can change this to `CUDAExecutionProvider` if you have a GPU and have\n",
        "# installed the CUDA runtime.\n",
        "ort_sess = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
        "for i in ort_sess.get_inputs():\n",
        "    print(f'input: {i.name}, {i.shape}, {i.type}')\n",
        "    \n",
        "print(f'Testing {count} rows')\n",
        "failed = 0\n",
        "for i in range(val_data.data.shape[0]):\n",
        "    data = val_data.data[i]    \n",
        "    expected = int(val_data.train_labels[i])\n",
        "\n",
        "    while len(data.shape) < 4:\n",
        "        data = np.expand_dims(data, axis=0)\n",
        "    outputs = ort_sess.run(None, {'input': data.astype(np.float32) / 255.0})\n",
        "    result = outputs[0]\n",
        "    index = np.argmax(result)\n",
        "    label = val_data.classes[index]\n",
        "    if expected != index:\n",
        "        # print(f'### Failed: {expected} and got {label}')\n",
        "        failed += 1\n",
        "          \n",
        "rate = (count - failed) * 100 / count\n",
        "print(f\"Failed {failed} out of {count} rows\")\n",
        "print(f'Inference pass rate is  {rate} %.')\n",
        "print(f'How does this compare with the training validation accuracy of {top_accuracy}')\n",
        "if np.isclose(rate, top_accuracy* 100, atol=0.1):\n",
        "    print('Success! The model is working as expected.')\n",
        "else:\n",
        "    print('The onnx runtime is giving different results.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbsphinx": "hidden"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "archai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "799abcba35f70097d02fca042963180a03ec3451fe1b7671ac5d22383cd0232c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
